{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b119bda1-7d77-4f79-b1e8-2147a920c9f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "# from DataCleaning import clean_text\n",
    "import csv\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be044c4f-126c-4d7b-a4d6-a834b81ea108",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done0\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "print(\"done0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "062dbb91-ac5d-40b9-a62d-a20ab073593f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[statistical modeling, probability, normal distribution, poisson distribution, survival analysis, hypothesis testing, bayesian inference, factor analysis, forecasting, markov chain, monte carlo, unstructured data, structured data, math, bayesian statistics, scientific method, statistics]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keyword_dict = pd.read_csv('skills_dictionary.csv')\n",
    "stats_words = [nlp(text) for text in keyword_dict['Statistics'].dropna(axis = 0)]\n",
    "NLP_words = [nlp(text) for text in keyword_dict['NLP'].dropna(axis = 0)]\n",
    "ML_words = [nlp(text) for text in keyword_dict['Machine Learning'].dropna(axis = 0)]\n",
    "DL_words = [nlp(text) for text in keyword_dict['Deep Learning'].dropna(axis = 0)]\n",
    "R_words = [nlp(text) for text in keyword_dict['R'].dropna(axis = 0)]\n",
    "python_words = [nlp(text) for text in keyword_dict['Python'].dropna(axis = 0)]\n",
    "Data_Engineering_words = [nlp(text) for text in keyword_dict['Data Engineering'].dropna(axis = 0)]\n",
    "Business = [nlp(text) for text in keyword_dict['Business'].dropna(axis = 0)]\n",
    "Software = [nlp(text) for text in keyword_dict['Software'].dropna(axis = 0)]\n",
    "Other = [nlp(text) for text in keyword_dict['Other'].dropna(axis = 0)]\n",
    "\n",
    "print(stats_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "026716aa-0e40-40f8-b998-bbe6d7bc36f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done2\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('Stats', None, *stats_words)\n",
    "matcher.add('NLP', None, *NLP_words)\n",
    "matcher.add('ML', None, *ML_words)\n",
    "matcher.add('DL', None, *DL_words)\n",
    "matcher.add('R', None, *R_words)\n",
    "matcher.add('Python', None, *python_words)\n",
    "matcher.add('DE', None, *Data_Engineering_words)\n",
    "matcher.add('Biz', None, *Business)\n",
    "matcher.add('SE', None, *Software)\n",
    "matcher.add('Other', None, *Other)\n",
    "\n",
    "print(\"done2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2ae6fb0-b953-4fc7-8832-be5bb881052f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    cleaning the text for the part of speech tagging\n",
    "    input a string, return a string\n",
    "    '''\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"[()<>/]\", ', ', text) # sub ()<>&/ to comma and space\n",
    "    text = re.sub(r\"&\", 'and', text) # sub ()<>&/ to comma and space\n",
    "    text = re.sub(r\"[?!]\", '. ', text) # sub ?! to dot and space\n",
    "\n",
    "    text = re.sub(\" [a-z0-9]+[\\.'\\-a-z0-9_]*[a-z0-9]+@\\w+\\.com\", \"\", text) # sub email address to dot\n",
    "    text = re.sub('[#\"\\']', '', text) # remove '#'\n",
    "\n",
    "    text = re.sub(\"e\\.g\\.\", '', text) # remove the 'e.g.'\n",
    "    text = re.sub(\"it’s\", 'it is', text)\n",
    "    text = re.sub(\"we’re\", 'we are', text)\n",
    "    text = re.sub(\"[\\t\\n\\r\\f\\v]+\", \". \", text) # remove \\n and \\r\n",
    "\n",
    "    text = re.sub(r'(?<=[.,])(?=[^\\s])', r' ', text) # add space after comma and dot\n",
    "\n",
    "\n",
    "    text = re.sub('\\W+\\.', '.', text) # remove the empty space before a dot\n",
    "    text = re.sub('\\W+\\,', ',', text) # remove the empty space before a comma\n",
    "    text = re.sub('[,\\.]+\\.+', '.', text) # sub multiple dots to one dot\n",
    "    text = re.sub(' +',' ',text) # replace multiple whitespace by one whitespace\n",
    "\n",
    "    return text\n",
    "\n",
    "def extract_entities(text):\n",
    "    d = [] \n",
    "    text = str(text)\n",
    "    text = clean_text(text)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'\n",
    "        span = doc[start : end]  # get the matched slice of the doc\n",
    "        d.append((rule_id, span.text))\n",
    "    return [i[1] for i,j in Counter(d).items()]\n",
    "\n",
    "def clean_entity(text):\n",
    "    text = re.sub('^\\W+', '', text)\n",
    "    text = re.sub('\\W+$', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    return text\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc539e-b038-4105-b940-7682c6b8ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Descriptions:   3%|▎         | 591/18261 [00:41<21:33, 13.66it/s]"
     ]
    }
   ],
   "source": [
    "        \n",
    "if __name__ == '__main__':\n",
    "    # read file\n",
    "    df = pd.read_csv(\"1.5_dataset.csv\")\n",
    "    descriptions = df['description'].values\n",
    "    \n",
    "    # extract entities in the file\n",
    "    entities = []\n",
    "        # Use tqdm to wrap the loop for progress tracking\n",
    "    for text in tqdm(descriptions, desc=\"Processing Descriptions\"):\n",
    "        for x in extract_entities(text):\n",
    "            entities.append(x)\n",
    "    word_could_dict=Counter(entities)\n",
    "    \n",
    "\n",
    "    \n",
    "    # write the top skills to csv file\n",
    "    with open('top_skills.csv','w') as csvfile:\n",
    "        fieldnames=['Skill', 'Count', 'Percentage']\n",
    "        writer=csv.writer(csvfile)\n",
    "        writer.writerow(fieldnames)\n",
    "        word_could_dict_sort = word_could_dict.most_common()\n",
    "        n = len(df)\n",
    "        for key, value in word_could_dict_sort:\n",
    "            writer.writerow([key] + [value] + [value / n]) \n",
    "\n",
    "    \n",
    "    # generate the word cloud\n",
    "    def purple_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n",
    "        return(\"hsl(260,100%%, %d%%)\" % np.random.randint(30, 70))\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, \n",
    "                          height=400, \n",
    "                          max_words=100,\n",
    "                          prefer_horizontal=1, \n",
    "                          background_color=\"white\").generate_from_frequencies(word_could_dict)\n",
    "    #change the color setting\n",
    "    wordcloud.recolor(color_func = purple_color_func)\n",
    "    \n",
    "    plt.figure(figsize=(100,50))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('wordcloud.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c1b73d-88c5-49e0-b67c-242bfe965b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done3\n"
     ]
    }
   ],
   "source": [
    "print(\"done3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2cb072-eb3a-416b-9142-b332481f5bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
